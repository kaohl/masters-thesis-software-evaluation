
TEMPORARY INFO DUMP


# Oranisation

evaluation.py:
1. Call into a number of independent experiment scripts

<experiment-script>.py (driver)
1. Specify experiment parameters
   - Experiment location
     - Root folder of the experiment, where all related resources are stored
   - Projects
     - A list of benchmarks to evaluate and,
       which dependencies of each benchmark
       should be built from source (and thus
       also exported to refactoring framework)
   - Refactoring parameters
     - Package, Class, Method filters (per project)
       - Manage these per project and compute a hash used as name to
         for a unique reuseable opportunity cache so that caches can
         be reused across experiments
         (PROJECT, HASH(filters)) => CACHE_PATH / { <refactoring type> }
     - Refactoring type
2. Call into 'experiment.py' to generate valid refactorings
   - Input: Experiment configuration
   - Output: a <data> folder with validated refactoring opportunities
3. Call into 'benchmark.py' to do benchmarking
   - Input: <data> folder with refactoring opportunities
   - Output: <results> folder with benchmark results per opportunity
4. Call into 'analysis.py' to analyse benchmark results
   - Input: <results> folder with benchmark results
   - Output: Visualisation of results


NOTE:
To support parallel generation of refactoring opportunities
we should be able to run these stages separately.
Each stage could have a command-line option that enables it:
  Stage-by-stage
  ./evaluation.py --refactor  --script batik-renamings --context <dir> --oppcache <opportunity cache dir>
  ./evaluation.py --benchmark --script batik-renamings --context <dir> --oppcache <opportunity cache dir>
  ./evaluation.py --analysis  --script batik-renamings --context <dir> --oppcache <opportunity cache dir>
  All
  ./evaluation.py --script batik-renamings --context <dir> --oppcache <opportunity cache dir>

  * Parameters are embedded in each script which makes them re-executable simply
    by specifying a context dir and a cache for opportunities
    - The alternative seems to be to track configurations in property files
      but it seems easier and more concise to pack all configuration into
      a python file that set up parameters for other parts of the program.
    * Note that the experiment parameters are specified at the evaluation script level

    # Parameter values per benchmark
    # We could store these in property files per benchmark
    # and then compute a set of valid configurations per
    # benchmark  

    parameter0 = { # JDK
      '<bm-name>' : [ <compatible value 0>, ..., <compatible value M> ],
      ...
    }

    parameter1 = { # JRE
      '<bm-name>' : [ <compatible value 0>, ..., <compatible value N> ]
    }

    parameter2 = { # Heap size
      '<bm-name>' : [ ... ]
    }
    ...

    configurations = []
    for bm in benchmarks:
      for p0 in parameter0[bm]:  # Add parameter filter to limit experiment to subset of parameters
        for p1 in parameter1[bm]:
          ...
            for pN in parameter2[bm]:
              configurations.append(bm, p1, p2, p3, ..., pN))

    for conf in configurations:
      for type in refactoring_types:
        

  We need to distinguish between
    - refactoring parameters (bm, steering-config, workspaces and assets)
    - compile time parameters (bm, SV, TV, JDK)
    - runtime parameters (JRE, HEAPSIZE)

  Refactoring parameters are handled by workspace setup
   - BM, VAR (exposed source trees)
     - Assets are generated by 'daivy.py'
   - Refactoring scope (packages, classes, methods)
   *** I think it is best if the refactoring wrapper script
       is independent of 'daivy' by only requiring a
       workspace path with assets and an export path
       for patches.       

  Compile time parameters are handled by 'daivy.py'
   - BM, JDK, Target Version, (source version is implied by bm), Patch import dir

  Runtime parameters are handled by 'benchmark.py'.
   - BM-deployment, TV, HEAP_SIZE, JRE

  We need to be able to run benchmarks with and without JFR and JIT logging configuration.

  ASSETS
    Source project assets are exported independently into a cache for reuse.
    Workspace assets is a collection of exported source project assets.
    Benchmark build-order gives available source dependencies for a benchmark workspace.

    BM  = dacapo:batik:1.0    # Example benchmark.
    SRC = batik-all-1.16, ... # Example source dependencies.

    *** We should be able to compute and cache all refactoring
        opportunities independently per project source tree.
        However, this cache can be lazily populated.
        * When we do this we should export all source dependencies
          otherwise we could miss indirect changes in an excluded
          source dependency which will fail the build.
          When the cache is computed it should be available
          to all subset of SRC options, though some may not
          compile for the same reason.

    *** I believe we can parameterize refactoring descriptors
        to avoid having to start up the framework just to change
        the new name in a renaming, if we want to answer RQs
        related to name length (code size, memory footprint).

  STEERING
    steering: (BM, JDK, JRE) => PKGs, CLSs, MTHs

      *** We might want to apply a default pre-compiled (reduced)
          source tree scope per project based on user input that
          is applied on top of the generated steering results to
          exclude uninteresting parts of the program source, or
          if we don't want to target those parts directly but
          indirectly.
            This allows us to study indirect effects or simply
          to prevent targeting of test code directly, which was
          the original reason for adding the scope filters.

      *** Note that generation/crafting of steering input is an
          independent process that we run once per (BM, JDK, JRE)
          tuple, and can then be reused as a settings folder for
          running the refactoring framework.

          Generated steering will be computed from JFR logs, JIT logging,
          and code coverage reports, or a subset of these.

          The default steering will be hand-crafted or generated
          from the source tree (all packages, classes, methods).

      *** The target scope is a subset of the exposed scope
      *** Steering generates a recommendation for the target scope
      *** SRCs defines the exposed scope of a benchmark
          - Subset of available source projects given by build-order
      *** (PKGs, CLSs, MTHs) defines the target scope

  REFACTOR/DATA
    refactor: (BM, SRC, SCOPE, TYPE, <type specific params>, OUTPUT_FOLDER) => refactored assets + metadata
      export: (BM, SRC, SCOPE) => workspace (configured)
      apply: (workspace, TYPE) => refactored assets + metadata
      write: (workspace, BM, SRC, SCOPE, TYPE, OUTPUT_FOLDER) => OUTPUT_FOLDER/{ DX { *.patch,  metadata { ... } } }

      *** The SCOPE parameter is a set of files that configures
          which packages, units, classes, and methods, should
          be considered for refactoring.

       *** The resulting data directory generated by 'write' will contain
           patches for all affected source projects, and refactoring meta
           data so that we know what refactoring action was applied, for
           analysis.

           - The steering parameter (SCOPE) is not needed to reproduce the
             experiment and will not need to be saved and propagated to
             analysis.

             Steering parameters only affect the search scope of the refactoring
             framework.

  BUILD
    build: (BM, PATCH, JKD, TARGET_VERSION) => deployment + metadata

  BENCHMARK
    benchmark: (deployment) => measurements + metadata

  ANALYSIS
    analysis: (measurements) => data visualisation
      - Process data into input files for ANOVA
      - Run ANOVA
      - Handle output

      Each datapoint is a configuration with associated measurements for measured metrics.

      Configuration (Parameters bound to explicit values)
        - BM
        - Patch info (text parameters)
          - Number of lines affected
          - Number of files affected (almost number of classes but no guarantee because of inner classes)
        - Refactoring metadata
          - Refactoring types may introduce additional parameters like "text span" and "bytes" for renamings
            or number of inlined calls for inline-method
        - JKD
        - JRE
        - HEAPSIZE
        - Target Version
        - Source Version
        - ...

      Measurements
        Single valued metrics
          <metric> = <value>
        Time-series metrics
          <metric> = [(<timestamp>, <value>)]


  *** Each stage will propagate metadata from previous stages and add their own.
      - Create a folder "metadata" in all input/output directories of process stages
        and accumulate all files from start to end of the overall process.

Pipeline:
data(...)              => data          = <output-dir>/{ R1, R2, ..., RN, metadata }
build(data)            => deployments   = <output-dir>/{ D1, D2, ..., DN, metadata }
benchmark(deployments) => measurements  = <output-dir>/{ B1, B2, ..., BN, metadata }
analysis(measurements) => visualisation = <output-dir>/{ V_RQ1, V_RQ2, ... }

The output and metadata folder from each stage should contain all info required to
execute the next stage given only the output of the previous stage without specifying
additional arguments. Meta data accumulates from stage to stage. This allows us to run
different stages on different machines and copy input/output between computers and avoids
potentially mixing up command-line arguments for different data sets. The first stage takes
all required input except for the chaining of input to output folders.

# Caching

- Source project assets
  - Generated once per project
- Workspaces (BM, SRC)
  - This cache is basically forced on us because we may need to start
    up eclipse and do manual actions to get a workspace running, the
    result can then be reused.
- Refactoring opportunities (already exists)
  - An opportunity cache is paired with a workspace where all source
    projects of the target benchmark has been exported and with the
    largest SCOPE possible. Workspaces that only consider a subset
    of those projects will be able to reuse such a cache.

assets/
  <project 1>/
    ...
  <project 2>/
    ...
  <project ...>/
    ...
  <project N>
    ...

workspaces/
  <bm + hash(SRC)>/
    ...

oppcache/ # Already exists
  ...


# Script IO

DATA
1. which benchmark?
2. which source projects to include in workspace? (exposed scope) <project names>
3. which steering configuration?                  (target  scope) <folder>
4. which refactoring types?                                       <strings>
5. How many data points?                                          <integer>
6. Where should data be written?                                  <folder>

BUILD
7. Which data folder?                    <use output of DATA stage>
8. Which JDKs?                           <strings> # JAVA_HOME values
9. Which Target Versions?                <strings>
10. Where should deployments be written? <folder>

BENCHMARK
11. Which data folder?                    <use output of BUILD stage>
12. Where should measurements be written? <folder>

ANALYSIS
13. Which data folder?                <use output of BENCHMARK stage>
14. Where should analysis be written? <folder>

To keep the framework independent of SDKMAN! we should
compile a configuration file with JAVA_HOME paths for
JDKs and JREs. Alternatively, call a 'compile.sh'
script that the user can replace with whatever is
needed. One such script can be an SDKMAN! integration.

# SDKMAN!

https://sdkman.io/usage
sdk install java ...
sdk use java 21.0.4-tem
sdk home java 21.0.4-tem

- TODO: Decide on a set of JDKs and JREs

jdk            = [('java', '21.0.4-tem')] # Pass to build
target_version = ['8', '11', '17']        # Pass to build
jre            = [('java', '21.0.4-tem')] # Pass to benchmark
heap_size      = ['64M', '128', '256M', '512M', '1G', '2G', '4G', '8G'] # Pass to benchmark

Use JDK to compile source code to target version (above or equal to source version)
and run in JRE (above or equal to target version).

CV := Compiler Version
RV := Runtime Version
SV := Source Version
TV := Target Version

SV := Minimum source version required by project (multi-release possible)
Applicable CVs (SV; CV >= SV)
Applicable TVs (SV, CV; TV >= SV and TV <= CV)
Applicable RVs (TV; RV >= TV)

Multi-release only affects the output artifact if we can't
compile all source trees because of missing compiler for a
specific version. However, we could call this build failure
if we want to be sure that the same artifact is used in all
benchmarks. Or we could define multiple versions of each
multi-release project where we only include source trees
up the the target version.

*** Generate parameter value compatibility matrix
    - Run each benchmark and workload once in each configuration and
      see if it compiles. Because our refactorings are unlikely to
      affect memory constraints it should suffice to run this test
      on the original benchmark.







  <context>/
    <script>/
    workspaces/
      <project X>/
        ...
      <project Y>/
        ...
      ...

  cache/
    <project>-<version>
      <refactoring config hash>/
        <refactoring type>/
          <valid opportunity A>/
            <meta-data>
            <patch files>
          <valid opportunity B>
            ...
          ...
        ...
      ...
    ...

experiment.py:
1. Set up eclipse workspaces and cache refactoring opportunities
   for affected projects
   - The 'packages.config', 'units.config', and 'methods.config' determine
     which projects, packages, and classes should be targeted.
2. Generate <n> refactorings into <data> folder
   1. Generate a refactoring opportunity patch
      - Note that "a patch" here is actually potentially multiple
        patches in the general case, one per affected source tree
        that is available to the refactoring framework, so the
        output is a folder of patch files for all affected source
        trees.
   2. Validate patch
      1. Build and run benchmark with opportunity patch
      2. If successful,
         then
           assume patch is valid and store patch to <data> folder
         else
           discard opportunity by adding a note to the opportunity
           cache that the opportunity is invalid.
      3. Continue to next opportunity.
3. Package <data> folder into an archive that can be
   moved to a benchmarking machine.

4. Benchmark
   1. Run to get dacapo harness metrics (runtime)
   2. Run again with flight recorder (once?)
   3. Potentially pre-process recordings
   4. Package output into a <results> folder or zip file that
      can later be retrieved from the benchmark machine
5. Process benchmark results
   - Could do this on my machine or on benchmark machine





The evaluation consists of a number of experiments.
An experiment is coded in a script called from the evaluation.
An experiment set up arguments and call into 'experiment.py'.
'experiment.py' call into the build and refactoring frameworks
to do specific tasks.
The experiment then 
framerefactoring framework and buisets up required workspaces based on experiment configuration



# Reuse refactoring opportunities
- Allow an experiment to specify a refactoring opportunity cache
  to skip regenerating per experiment (some experiments will be
  the same projects but with different variable projects which
  could mean that some cached opportinities for some projects
  could be reused).
