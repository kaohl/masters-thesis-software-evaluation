#!/bin/env python3

import argparse
import configuration
import os
from pathlib import Path
from random import randrange
import shutil
import tempfile

import run_benchmark as bm_script
import steering
import workspace     as ws_script

# File system layout
#
# Each benchmark workload is a standalone experiment.
# Because, each workload may have different hot methods
# and configuration limits (heap size; stack size; etc.).
# An experiment is therefore a set of properties and parameters
# for a given benchmark and workload.
# We record the full set of configuration and parameters per data point
# (excluding refactoring configurationso that we can 
# 
# NOTE: We don't save the information to regenerate the exact sequence of
#       refactorings. We do store the transformation as patches and measurements
#       should be reproducible given a patch.
#
#       properties.txt
#         # TODO: Is properties needed when we have BM and WORKLOAD names in path?
#         # Also, we don't record refactoring arguments (seeds, length, etc.) because
#         # experiments are reproducible from patches.
#         # The reason for this is that refactorings can be generated in batches with
#         # random seeds to avoid hitting the same refactorings over and over again,
#         # and tracking the history for each run of the refactoring generator is
#         # overkill when all we need to save is the patch.
#
# NOTE: Exported resources are stored inside each prepared workspace and can be reused.
#
# experiments/<x>/
#   benchmark-log.txt
#     - Write "DONE <config hash>" when benchmark is complete for specified configuration.
#     - Save to track finished benchmark configurations
#       - Makes parameter value lists extensible
#       - Makes it possible to terminate the experiment program and resume on restart
#       - Makes it possible to extract stats from the data tree without triggering recomputation of benchmarks
#   workloads
#     jacop/default/
#       parameters.txt
#         - Text file listing experiment parameters and values
#         - Experiment loops over all configurations; extensible value lists.
#       config/
#         packages.config  # Only required for random refactorings; not with hot method steering.
#         units.config     # Only required for random refactorings; not with hot method steering.
#         methods.config   # Generated by steering script.
#         variable.config  # I believe this one is generated on export?
#     batik/small/
#       parameters.txt
#       config/
#         ...
#     ...
#   workspace/<bm>/<workload>     # Eclipse workspace.
#     ...
#   data/                            # Refactoring output.
#     <bm>/
#       <workload>/
#         <tmp ...>/                     # A specific refactoring.
#           stats/                       # Benchmark execution results.
#             <configuration 0>/         # A specific configuration.
#               configuration.txt        # Experiment parameter values (configuration vector).
#               <tmp ...>/               # A specific execution of "configuration 0".
#                 <measurements>         # Benchmark execution measurements.
#


# 1. Scan experiment folder for benchmark workloads <experiments>/<x>/workload/<bm>/<workload>/parameters.txt
# 2. For each workload, generate (or copy cached files) steering and create <experiments>/<x>/steering/<bm>.<workload>
# 3. For each workload, generate a workspace with oppcache based on workload steering
# 4. Generate refactorings                        (separate invokation)
# 5. Benchmark refactorings in all configurations (separate invokation)
#
# When we refactor we should be able to run with the experiment tree on the USB.
# However, we we deploy benchmarks we should probably move benchmark data onto
# the machine disk to not communicate with the USB during benchmarks.
#
# (install-from-usb.py) Deploying the evaluation on benchmark machines
# 1. Pull <evaluation> and <daivy> repos onto the benchmark machines into assigned locations
#    - We could store gitrepos on USB and pull from there which simplify updating the framework
#      on the benchmark machine (simply update the USB and the do a new pull on the machine via USB)
# 2. Move a pre-populated ivy-cache from the USB into <daivy>
# 3. Move a pre-defined experiment folder into <evaluation> (contains configurations and potentially some refactorings)
# 4. Set DAIVY_HOME and start the 'evaluation.py' script
#
# It would be good if the 'install-from-usb.py' script works incrementally
# so that it is easy to update the installation with experiments and additions
# to ivy-cache and to daivy. This is easily done by simply removing the current
# deployed folders and then moving updated folders from the USB which can then
# be deleted from the USB if needed to make room for data. The git repos will
# be incremental by default. Of course, we could make the ivy-cache a local repo
# on the USB to also handle incremental updates of the ivy-cache.

def x_folder(args):
    return Path(args.x_location) / args.x

def get_workloads(args):
    bmwl = []
    for dir_1, bms, files_1 in os.walk(x_folder(args) / 'workloads'):
        for bm in bms:
            for dir_2, workloads, files_2 in os.walk(x_folder(args) / 'workloads' / bm):
                for workload in workloads:
                    bmwl.append((bm, workload))
                break
        break
    return bmwl

def get_workload_configuration(args, bm, workload):
    return configuration.Configuration().load(x_folder(args) / 'workloads' / bm / workload / 'parameters.txt')

def add_workload_steering(args, bm, workload, workspace_src):
    target  = workspace_src / 'methods.config'
    if target.exists():
        return
    methods = steering.get_all_sampled_methods(bm, workload)
    with open(target, 'w') as f:
        for method in methods:
            if method.find(bm) != -1:
                f.write(method + os.linesep)

def add_workspace_configuration(args, bm, workload, workspace):
    src    = workspace / 'assets/src'
    config = x_folder(args) / 'workloads' / bm / workload / 'config'
    src.mkdir(parents = True, exist_ok = True)
    add_workload_steering(args, bm, workload, src)
    for dir, folders, files in os.walk(config):
        for file in files:
            shutil.copy2(Path(dir) / file, src / file)

def create_workspace(args, bm, workload):
    workspace = x_folder(args) / 'workspaces' / bm / workload / 'workspace'
    add_workspace_configuration(args, bm, workload, workspace)
    ws_script.create_workspace_in_location("dacapo:{}:1.0".format(bm), workspace)

def create_workspaces(args):
    for bm, workload in get_workloads(args):
        create_workspace(args, bm, workload)

# Usage:
#  ./evaluation.py --x <ex> --create
#
def create(args):
    create_workspaces(args)

# Usage:
#  ./evaluation.py --bm <bm> --x <ex> --tag <tag> --refactor --type <refactoring type> [ refactoring options ]
#
def refactor(args, proc_id):
    if not 'bm' in args:
        raise ValueError("Please specify a benchmark.")

    if not 'workload' in args:
        raise ValueError("Please specify a workload.")

    bm        = args.bm
    workload  = args.workload
    workspace = x_folder(args) / 'workspaces' / bm / workload / 'workspace'
    data      = x_folder(args) / 'data' / bm / workload

    # TODO: Allow user to specify refactoring options on command line.
    options = configuration.get_random_refactoring_configuration()

    ws_script.refactor(workspace, data, options, proc_id)

def _benchmark_data(bm, workload, data_location, configuration, capture_flight_recording = True):
    store              = data_location / 'stats' / configuration.id()
    jfr_save           = store / 'flight.jfr'
    metrics_save       = store / 'metrics.txt'
    configuration_save = store / 'configuration.txt'

    clean = True
    with tempfile.TemporaryDirectory(delete = False, dir = 'temp') as location:
        temp_location = Path(location)
        deploy_dir    = temp_location / 'deployment'
        jfr_file      = temp_location / 'flight.jfr'
        deploy_dir.mkdir()
        bm_script.deploy_benchmark(bm, clean, deploy_dir)

        # Capture execution time with flight recording disabled.
        exectime = bm_script.run_benchmark(bm, deploy_dir, { '-size' : workload }, False, None)

        store.mkdir(parents = True, exist_ok = True)

        with open(metrics_save, 'w') as f:
            f.write("EXECUTION_TIME=" + str(exectime) + os.linesep)

        configuration.store(configuration_save)

        # ATTENTION
        # The captured flight recording is not for the benchmark
        # run that produced the captured execution time.

        if capture_flight_recording:
            print("Running again to capture flight recording")
            bm_script.run_benchmark(bm, deploy_dir, { '-size' : workload }, True, str(jfr_file))
            shutil.copy2(jfr_file, jfr_save)

#def _benchmark_all(data_location, configuration):
#    for root, folders, files in os.walk(data_location):
#        for folder in folders:
#            _benchmark_data(Path(root) / folder, configuration)
#        break

# TODO: Not sure caching the plan is needed.
def create_benchmark_execution_plan(args):
    plan           = []
    #configurations = dict()
    for (bm, workload) in get_workloads(args):
        config = get_workload_configuration(args, bm, workload)
        for c in config.get_all_combinations():
            #configurations[c.id()] = c
            for dir, folders, files in os.walk(x_folder(args) / 'data' / bm / workload):
                for refactoring in folders:
                    stats_c = Path(dir) / refactoring / 'stats' / c.id()
                    if not stats_c.exists():
                        plan.append((bm, workload, refactoring, c))
                break
    #plan_file = x_folder(args) / 'benchmark-execution-plan.txt'
    #with open(plan_file, 'w') as f:
    #    for (bm, workload, configuration) in plan:
    #        f.write(','.join([bm, workload, configuration.id()]))
    return plan#, configurations

def get_benchmark_execution_plan(args):
    return create_benchmark_execution_plan(args)

# Usage:
#   ./evaluation.py --bm <bm> --x <ex> --tag <tag> --benchmark [--data <tmp...>]
#
def benchmark(args):
    # TODO: Do we need a better strategy to avoid generating duplicates of refactorings?
    # TODO: Handle benchmark failure.
    i = 0 # TODO: Add as input parameter.
    for (bm, workload, refactoring, configuration) in get_benchmark_execution_plan(args):
        data_location = x_folder(args) / 'data' / bm / workload / refactoring
        _benchmark_data(bm, workload, data_location, configuration)
        i = i + 1
        if i == 5:
            break

def report(args):
    for (bm, workload) in get_workloads(args):
        for dir, refactorings, files in os.walk(x_folder(args) / 'data' / bm / workload):
            for refactoring in refactorings:
                for dir_2, ids, files_2 in os.walk(Path(dir) / refactoring / 'stats'):
                    for id in ids:
                        # TODO: Investigate how input should be formatted for statistics tools.
                        #       Consider letting the user specify the column headers (keys as CSV) and then print CSV rows.
                        config  = configuration.Configuration().load(Path(dir) / refactoring / 'stats' / id / 'configuration.txt')
                        metrics = configuration.Metrics().load(Path(dir) / refactoring / 'stats' / id / 'metrics.txt')
                        print(config._values, metrics._values)
                    break
            break

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--x', required = True,
        help = "The experiment name.")
    parser.add_argument('--x-location', required = False, default = 'experiments',
        help = "Location where experiments are stored. Defaults to 'experiments'.")
    parser.add_argument('--bm', required = False,
        help = "Benchmark name")
    parser.add_argument('--workload', required = False,
        help = "Benchmark workload")
    parser.add_argument('--create', required = False, action = 'store_true',
        help = "Create experiment workspace from specified template")
    parser.add_argument('--benchmark', required = False, action = 'store_true',
        help = "Benchmark refactoring(s)")
    parser.add_argument('--data', required = False,
        help = "A specific refactoring folder name") 
    parser.add_argument('--refactor', required = False, action = 'store_true',
        help = "Refactor specified experiment workspace")
    parser.add_argument('--type', required = False,
        help = "Refactoring type")
    parser.add_argument('--report', required = False, action = 'store_true',
        help = "Print statistics")
    args = parser.parse_args()

    if args.create:
        create(args)
    elif args.benchmark:
        benchmark(args)
    elif args.refactor:
        refactor(args, 0)
    elif args.report:
        report(args)
    else:
        parser.print_help()

